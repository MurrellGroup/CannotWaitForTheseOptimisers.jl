var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = CannotWaitForTheseOptimisers","category":"page"},{"location":"#CannotWaitForTheseOptimisers","page":"Home","title":"CannotWaitForTheseOptimisers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for CannotWaitForTheseOptimisers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [CannotWaitForTheseOptimisers]","category":"page"},{"location":"#CannotWaitForTheseOptimisers.Apollo","page":"Home","title":"CannotWaitForTheseOptimisers.Apollo","text":"Apollo(opt::AdamW = AdamW(), r::Function = dim -> ceil(Int, sqrt(dim)); u = 100, sort_dims = true)\nApollo(η::Real, args...; kw...)\nApollo(arg, rank::Int; kw...)\nApollo(η::Real, rank::Int; kw...)\n\nApollo optimizer from Zhu et al. (https://arxiv.org/abs/2412.05270). Tracks moments in a low-rank subspace, aiming for Adam-like behavior with minimal additional memory usage. First argument can be an AdamW optimizer, or a learning rate (which will use the default AdamW optimizer with that learning rate). Second argument can be a rank, or a function to compute the rank from the second dimension (or the product of all dims > 1) of the weight matrix (or tensor).\n\n\n\n\n\n","category":"type"},{"location":"#CannotWaitForTheseOptimisers.GradNormControl","page":"Home","title":"CannotWaitForTheseOptimisers.GradNormControl","text":"GradNormControl(accumulator, τ = 1.1; epsilon = 1e-8, lb = 0.1, throw = true, scale = true, clipreportthresh = Inf)\n\nNormGrowthCap with additional control, accumulation, and reporting options. accumulator must be an array of Float64 with two elements, which is where the unscaled and scaled gradient norms are added into, allowing you to monitor the sum of the norms. It is your job to print/reset this.\n\n\n\n\n\n","category":"type"},{"location":"#CannotWaitForTheseOptimisers.Muon","page":"Home","title":"CannotWaitForTheseOptimisers.Muon","text":"Muon(opt = AdamW(eta = 0.0003, beta = (0.9,0.95), lambda = 0.01), η = 0.02, μ = 0.95, λ = 0.01, fallback = Returns(false))\nMuon(; [opt, eta, mu, lambda, fallback])\n\nMuon - MomentUm Orthogonalized by Newton-schulz (https://github.com/KellerJordan/Muon)\n\nMuon internally runs standard SGD-momentum, and then performs an orthogonalization post-processing step, in which each 2D parameter's update is replaced with the nearest orthogonal matrix using Newton-Schulz iteration.\n\nParameters\n\nFallback optimizer (opt): Optimizer to use for 1D parameters or when the fallback function returns true\nLearning rate (η == eta): Amount by which gradients are discounted before updating the weights\nMomentum (μ == mu): Controls the acceleration of gradient descent in the prominent direction\nWeight decay (λ == lambda): Controls the strength of L_2 regularisation.\nFallback function (fallback): Function to control when, in addition to 1D arrays, the fallback optimizer should be used. Will be passed the parameter array and must return a boolean.\n\nNote: Works best with large batch sizes and may not be suitable for fine-tuning. In nanoGPT speedrun experiments, Muon is used for the internal layer >2D weights, and AdamW is used for the 1D weights, embeddings, and heads.\n\nOptimisers.adjust!(optimiser_state, η::Real) will adjust the fallback optimizer's eta to η * (opt.eta / eta), and Muon's eta to η, preserving their ratio, but Optimisers.adjust!(optimiser, eta = η) will only adjust Muon's learning rate (allowing you to adjust the fallback optimizer's learning rate separately).\n\n\n\n\n\n","category":"type"},{"location":"#CannotWaitForTheseOptimisers.NormGrowthCap","page":"Home","title":"CannotWaitForTheseOptimisers.NormGrowthCap","text":"NormGrowthCap(τ = 1.01; ϵ = 1e-8, lb = 1e-7, throw = true, scale = true)\n\nGradient norm growth limiter. τ controls the maximum that the gradient norm can grow from one step to the next, such that if ||dx||/||dx_prev|| > τ & ||dx|| > lb, then dx = dx * τ*||dx_prev||/(||dx||+ϵ) Inspired by Chen et al. and used with Apollo in Zhu et al., but with Optimisers.jl this will apply per-tensor instead of per-model. This implementation also introduces lb as a hard minimum on the gradient norm threshold, and never rescales grads below this, preventing a tensor from getting \"trapped\" near zero. This can be a fixed min, or scaled by the square root of the number of parameters in the tensor (with scale = true).\n\n\n\n\n\n","category":"type"}]
}
